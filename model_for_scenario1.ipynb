{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real and Synthetic data raw without balancing and data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries and read data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctgan import CTGAN\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define data for each scenarios with data paths\n",
    "synthetic_data = pd.read_csv(\"../Data/synthetic_data_scenario1.csv\") \n",
    "feature_df = pd.read_csv(\"../Data/preprocessed_df_scenario1.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF without Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC-AUC score: 0.5129\n",
      "Test Accuracy: 0.8257\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       520\n",
      "           1       0.83      1.00      0.90      2480\n",
      "\n",
      "    accuracy                           0.83      3000\n",
      "   macro avg       0.41      0.50      0.45      3000\n",
      "weighted avg       0.68      0.83      0.75      3000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[   0  520]\n",
      " [   3 2477]]\n",
      "False Positive Rate: 1.0\n",
      "TN, FP, FN, TP: (0, 520, 3, 2477)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X, y = synthetic_data.drop(\"Label\", axis=1), synthetic_data.Label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the data (fit on training set and transform both training and test sets)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and fit the RandomForestClassifier model with default parameters\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred_proba = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "y_test_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Compute metrics\n",
    "roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "class_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Test ROC-AUC score: {roc_auc:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Extract True Negatives, False Positives, False Negatives, and True Positives from the confusion matrix\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Calculate False Positive Rate (FPR)\n",
    "fpr = fp / (fp + tn)\n",
    "print(f\"False Positive Rate: {fpr}\")\n",
    "print(f\"TN, FP, FN, TP: {tn, fp, fn, tp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.80      1.00      0.89        48\n",
      "\n",
      "    accuracy                           0.80        60\n",
      "   macro avg       0.40      0.50      0.44        60\n",
      "weighted avg       0.64      0.80      0.71        60\n",
      "\n",
      "False Positive Rate (FPR): 1.0000\n",
      "\n",
      "Confusion Matrix Scores:\n",
      "True Positives (TP): 48\n",
      "True Negatives (TN): 0\n",
      "False Positives (FP): 12\n",
      "False Negatives (FN): 0\n"
     ]
    }
   ],
   "source": [
    "#TEST ON REAL DATASET, for each scenario, change test data file name correspondingly. for each model, change rf_model from 1 to 5.\n",
    "\n",
    "#evaluate the model on the test data\n",
    "X_test, y_test = feature_df.drop(\"Label\", axis=1), feature_df.Label\n",
    "\n",
    "#scale the real test data using the same scaler\n",
    "X_real_scaled = scaler.transform(feature_df.drop(\"Label\", axis=1))\n",
    "print(classification_report(y_test, rf_model.predict(X_real_scaled)))\n",
    "#generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, rf_model.predict(X_real_scaled))\n",
    "\n",
    "#extract confusion matrix values\n",
    "TN, FP, FN, TP = cm.ravel()  # Unpack the confusion matrix\n",
    "\n",
    "#calculate the False Positive Rate (FPR)\n",
    "FPR = FP / (FP + TN)\n",
    "print(f\"False Positive Rate (FPR): {FPR:.4f}\")\n",
    "\n",
    "#print confusion matrix values with descriptions\n",
    "print(\"\\nConfusion Matrix Scores:\")\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB without optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC-AUC score: 0.5006\n",
      "Test Accuracy: 0.8007\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.05      0.08       520\n",
      "           1       0.83      0.96      0.89      2480\n",
      "\n",
      "    accuracy                           0.80      3000\n",
      "   macro avg       0.51      0.50      0.48      3000\n",
      "weighted avg       0.72      0.80      0.75      3000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  26  494]\n",
      " [ 104 2376]]\n",
      "False Positive Rate: 0.95\n",
      "TN,FP,FN,TP: (26, 494, 104, 2376)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report, confusion_matrix, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X, y = synthetic_data.drop(\"Label\", axis=1), synthetic_data.Label\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_scaled)\n",
    "X_test_scaled = scaler.transform(X_test_scaled)\n",
    "\n",
    "# Initialize and fit the XGBClassifier model with default parameters\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "y_test_pred = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Compute metrics\n",
    "roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "class_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Test ROC-AUC score: {roc_auc:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming `y_true` are the true labels and `y_pred` are the predicted labels\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "# Calculate False Positive Rate\n",
    "fpr = fp / (fp + tn)\n",
    "print(f\"False Positive Rate: {fpr}\")\n",
    "print(f\"TN,FP,FN,TP: {tn,fp,fn,tp}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.08      0.15        12\n",
      "           1       0.81      1.00      0.90        48\n",
      "\n",
      "    accuracy                           0.82        60\n",
      "   macro avg       0.91      0.54      0.53        60\n",
      "weighted avg       0.85      0.82      0.75        60\n",
      "\n",
      "False Positive Rate (FPR): 0.9167\n",
      "\n",
      "Confusion Matrix Scores:\n",
      "True Positives (TP): 48\n",
      "True Negatives (TN): 1\n",
      "False Positives (FP): 11\n",
      "False Negatives (FN): 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "X_test, y_test = feature_df.drop(\"Label\", axis=1), feature_df.Label\n",
    "# Scale the real test data using the same scaler\n",
    "X_real_scaled = scaler.transform(feature_df.drop(\"Label\", axis=1))\n",
    "\n",
    "print(classification_report(y_test, xgb_model.predict(X_real_scaled)))\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, xgb_model.predict(X_real_scaled))\n",
    "\n",
    "# Extract confusion matrix values\n",
    "TN, FP, FN, TP = cm.ravel()  # Unpack the confusion matrix\n",
    "\n",
    "# Calculate the False Positive Rate (FPR)\n",
    "FPR = FP / (FP + TN)\n",
    "print(f\"False Positive Rate (FPR): {FPR:.4f}\")\n",
    "\n",
    "# Print confusion matrix values with descriptions\n",
    "print(\"\\nConfusion Matrix Scores:\")\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOG REG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Without optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC-AUC score: 0.5475\n",
      "Test Accuracy: 0.8267\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       520\n",
      "           1       0.83      1.00      0.91      2480\n",
      "\n",
      "    accuracy                           0.83      3000\n",
      "   macro avg       0.41      0.50      0.45      3000\n",
      "weighted avg       0.68      0.83      0.75      3000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[   0  520]\n",
      " [   0 2480]]\n",
      "False Positive Rate: 1.0\n",
      "TN,FP,FN,TP: (0, 520, 0, 2480)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report, confusion_matrix, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X, y = synthetic_data.drop(\"Label\", axis=1), synthetic_data.Label\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_scaled))\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_scaled))\n",
    "\n",
    "# Initialize and fit the Logistic Regression model with default parameters\n",
    "Logreg_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "Logreg_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred_proba = Logreg_model.predict_proba(X_test_scaled)[:, 1]\n",
    "y_test_pred = Logreg_model.predict(X_test_scaled)\n",
    "\n",
    "# Compute metrics\n",
    "roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "class_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Test ROC-AUC score: {roc_auc:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming `y_true` are the true labels and `y_pred` are the predicted labels\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "# Calculate False Positive Rate\n",
    "fpr = fp / (fp + tn)\n",
    "print(f\"False Positive Rate: {fpr}\")\n",
    "print(f\"TN,FP,FN,TP: {tn,fp,fn,tp}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.80      1.00      0.89        48\n",
      "\n",
      "    accuracy                           0.80        60\n",
      "   macro avg       0.40      0.50      0.44        60\n",
      "weighted avg       0.64      0.80      0.71        60\n",
      "\n",
      "False Positive Rate (FPR): 1.0000\n",
      "\n",
      "Confusion Matrix Scores:\n",
      "True Positives (TP): 48\n",
      "True Negatives (TN): 0\n",
      "False Positives (FP): 12\n",
      "False Negatives (FN): 0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "\n",
    "X_test, y_test = feature_df.drop(\"Label\", axis=1), feature_df.Label\n",
    "# Scale the real test data using the same scaler\n",
    "X_real_scaled = scaler.transform(feature_df.drop(\"Label\", axis=1))\n",
    "\n",
    "print(classification_report(y_test, Logreg_model.predict(X_real_scaled)))\n",
    "\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, Logreg_model.predict(X_real_scaled))\n",
    "\n",
    "# Extract confusion matrix values\n",
    "TN, FP, FN, TP = cm.ravel()  # Unpack the confusion matrix\n",
    "\n",
    "# Calculate the False Positive Rate (FPR)\n",
    "FPR = FP / (FP + TN)\n",
    "print(f\"False Positive Rate (FPR): {FPR:.4f}\")\n",
    "\n",
    "# Print confusion matrix values with descriptions\n",
    "print(\"\\nConfusion Matrix Scores:\")\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dec tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dec tree default params, without optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC-AUC score: 0.4873\n",
      "Test Accuracy: 0.6800\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.19      0.17       520\n",
      "           1       0.82      0.78      0.80      2480\n",
      "\n",
      "    accuracy                           0.68      3000\n",
      "   macro avg       0.49      0.49      0.49      3000\n",
      "weighted avg       0.71      0.68      0.69      3000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 100  420]\n",
      " [ 540 1940]]\n",
      "False Positive Rate: 0.8076923076923077\n",
      "TN,FP,FN,TP: (100, 420, 540, 1940)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, classification_report, confusion_matrix, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X, y = synthetic_data.drop(\"Label\", axis=1), synthetic_data.Label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test))\n",
    "\n",
    "# Initialize and fit the Decision Tree model with default parameters\n",
    "DT_model = DecisionTreeClassifier(random_state=42)\n",
    "DT_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred_proba = DT_model.predict_proba(X_test_scaled)[:, 1] if hasattr(DT_model, \"predict_proba\") else DT_model.predict(X_test_scaled)\n",
    "y_test_pred = DT_model.predict(X_test_scaled)\n",
    "\n",
    "# Compute metrics\n",
    "roc_auc = roc_auc_score(y_test, y_test_pred_proba) if hasattr(DT_model, \"predict_proba\") else None\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "class_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Test ROC-AUC score: {roc_auc:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming `y_true` are the true labels and `y_pred` are the predicted labels\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "# Calculate False Positive Rate\n",
    "fpr = fp / (fp + tn)\n",
    "print(f\"False Positive Rate: {fpr}\")\n",
    "print(f\"TN,FP,FN,TP: {tn,fp,fn,tp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.25      0.30        12\n",
      "           1       0.83      0.90      0.86        48\n",
      "\n",
      "    accuracy                           0.77        60\n",
      "   macro avg       0.60      0.57      0.58        60\n",
      "weighted avg       0.74      0.77      0.75        60\n",
      "\n",
      "False Positive Rate (FPR): 0.7500\n",
      "\n",
      "Confusion Matrix Scores:\n",
      "True Positives (TP): 43\n",
      "True Negatives (TN): 3\n",
      "False Positives (FP): 9\n",
      "False Negatives (FN): 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_test, y_test = feature_df.drop(\"Label\", axis=1), feature_df.Label\n",
    "# Scale the real test data using the same scaler\n",
    "X_real_scaled = scaler.transform(feature_df.drop(\"Label\", axis=1))\n",
    "\n",
    "print(classification_report(y_test, DT_model.predict(X_real_scaled)))\n",
    "\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, DT_model.predict(X_real_scaled))\n",
    "\n",
    "# Extract confusion matrix values\n",
    "TN, FP, FN, TP = cm.ravel()  # Unpack the confusion matrix\n",
    "\n",
    "# Calculate the False Positive Rate (FPR)\n",
    "FPR = FP / (FP + TN)\n",
    "print(f\"False Positive Rate (FPR): {FPR:.4f}\")\n",
    "\n",
    "# Print confusion matrix values with descriptions\n",
    "print(\"\\nConfusion Matrix Scores:\")\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5806, number of negative: 1194\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 166\n",
      "[LightGBM] [Info] Number of data points in the train set: 7000, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.829429 -> initscore=1.581583\n",
      "[LightGBM] [Info] Start training from score 1.581583\n",
      "Test ROC-AUC score: 0.5280\n",
      "Test Accuracy: 0.8257\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.00      0.00       512\n",
      "           1       0.83      1.00      0.90      2488\n",
      "\n",
      "    accuracy                           0.83      3000\n",
      "   macro avg       0.45      0.50      0.45      3000\n",
      "weighted avg       0.70      0.83      0.75      3000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[   1  511]\n",
      " [  12 2476]]\n",
      "False Positive Rate: 0.998046875\n",
      "TP,TN,FP,FN: (2476, 1, 511, 12)\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X, y = synthetic_data.drop(\"Label\", axis=1), synthetic_data[\"Label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Scale the features once for both training and testing sets\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and fit the LGBMClassifier model with default parameters\n",
    "LGBM_model = LGBMClassifier(random_state=42, n_jobs=-1)\n",
    "LGBM_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred_proba = LGBM_model.predict_proba(X_test_scaled)[:, 1]\n",
    "y_test_pred = LGBM_model.predict(X_test_scaled)\n",
    "\n",
    "# Compute metrics\n",
    "roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "class_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Test ROC-AUC score: {roc_auc:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming `y_true` are the true labels and `y_pred` are the predicted labels\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "# Calculate False Positive Rate\n",
    "fpr = fp / (fp + tn)\n",
    "print(f\"False Positive Rate: {fpr}\")\n",
    "print(f\"TP,TN,FP,FN: {tp,tn,fp,fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.80      1.00      0.89        48\n",
      "\n",
      "    accuracy                           0.80        60\n",
      "   macro avg       0.40      0.50      0.44        60\n",
      "weighted avg       0.64      0.80      0.71        60\n",
      "\n",
      "False Positive Rate (FPR): 1.0000\n",
      "\n",
      "Confusion Matrix Scores:\n",
      "True Positives (TP): 48\n",
      "True Negatives (TN): 0\n",
      "False Positives (FP): 12\n",
      "False Negatives (FN): 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "X_test, y_test = feature_df.drop(\"Label\", axis=1), feature_df.Label\n",
    "# Scale the real test data using the same scaler\n",
    "X_real_scaled = scaler.transform(feature_df.drop(\"Label\", axis=1))\n",
    "\n",
    "print(classification_report(y_test, LGBM_model.predict(X_real_scaled)))\n",
    "\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, LGBM_model.predict(X_real_scaled))\n",
    "\n",
    "# Extract confusion matrix values\n",
    "TN, FP, FN, TP = cm.ravel()  # Unpack the confusion matrix\n",
    "\n",
    "# Calculate the False Positive Rate (FPR)\n",
    "FPR = FP / (FP + TN)\n",
    "print(f\"False Positive Rate (FPR): {FPR:.4f}\")\n",
    "\n",
    "# Print confusion matrix values with descriptions\n",
    "print(\"\\nConfusion Matrix Scores:\")\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
